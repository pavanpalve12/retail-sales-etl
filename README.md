
# Retail Sales ETL Pipeline (Pandas â†’ SQLite)

An **end-to-end, production-style ETL pipeline** that ingests raw retail CSV datasets, applies structured data conditioning and dimensional modeling using **Pandas**, and loads analyticsâ€‘ready **fact and dimension tables** into **SQLite**, backed by a full **logging and metadata control plane**.

This project is designed to mirror **real-world data engineering systems**, scaled down to a local, frameworkâ€‘free environment.

---

## ğŸ§  Project Objectives

- Build a **clean, modular ETL system** (Extract â†’ Transform â†’ Load)
- Separate **data processing**, **logging**, and **metadata control**
- Model data into **fact and dimension tables**
- Enable **SQL-based analytics**
- Design for **reruns, debuggability, and future extensibility**

---

## ğŸ—ï¸ High-Level Architecture

### Logical Components

- **Raw Data Landing** (CSV files on local filesystem)
- **ETL Platform**
  - Extract
  - Transform Phase 1 (Conditioning)
  - Transform Phase 2 (Modeling)
  - Load
  - Post-load Validation
- **Log & Metadata Database** (SQLite)
- **Data Warehouse Database** (SQLite)

---

## ğŸ” ETL Flow

```
Raw CSVs
   â†“
Extract
   â†“
Transform â€“ Conditioning
   â†“
Transform â€“ Modeling
   â†“
Load â†’ Warehouse
   â†“
Post-load Validation
```

Each stage emits logs and interacts with metadata tables.

---

## ğŸ“¦ Data Sources

Raw CSV datasets (example):

- sales.csv
- customers.csv
- products.csv
- stores.csv

All raw files are treated as **immutable inputs**.

---

## ğŸ§© Transform Strategy

### Phase 1 â€” Data Conditioning
Purpose: make data **safe and consistent**

- Handle missing values
- Type conversions (dates, numerics, categories)
- Deduplication
- Basic sanity checks

Output: clean, source-shaped DataFrames

---

### Phase 2 â€” Data Modeling
Purpose: make data **analytics-ready**

- Build dimension tables
- Define fact table grain
- Feature engineering (e.g. revenue, discounts)
- Enforce logical foreign-key integrity

Output: fact and dimension DataFrames

---

## ğŸ—„ï¸ Warehouse Schema

### Dimension Tables
- `customers_dim`
- `products_dim`
- `stores_dim`
- `date_dim`

### Fact Table
- `sales_fact`  
  *Grain: one row per sales transaction*

---

## ğŸ“Š Logging Design

### ETL Run Log
Tracks pipeline-level executions.

Example:
- pipeline start / end
- success or failure
- error messages

### ETL Stage Log
Tracks individual stage events.

Stages logged:
- EXTRACT
- TRANSFORM_P1
- TRANSFORM_P2
- LOAD
- VALIDATION

Logs are **append-only** and never mutated.

---

## ğŸ§  Metadata Control Plane

Metadata tables define **how the system behaves**.

### Pipeline Metadata
Defines:
- pipeline name
- source
- load strategy
- activation state

### Table Metadata
Defines:
- table name
- grain
- load type
- last load timestamp
- row counts

### Pipeline â†” Table Mapping
Defines:
- which pipeline loads which tables
- load order
- table role (dimension / fact)

> ETL logic reads metadata **before execution** and updates it **only on success**.

---

## âœ… Validation Layer

Executed **after loading** and before analytics access.

Checks include:
- row count reconciliation
- null checks on critical columns
- duplicate key detection
- referential integrity validation

Validation results are logged as ETL stages.

---

## ğŸ“‚ Project Structure

```
retail-sales-etl/
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform_p1.py
â”‚   â”œâ”€â”€ transform_p2.py
â”‚   â”œâ”€â”€ load.py
â”‚   â””â”€â”€ validate.py
â”‚
â”œâ”€â”€ runner/
â”‚   â””â”€â”€ pipeline_runner.py
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ metadata_ddl.sql
â”‚   â””â”€â”€ warehouse_ddl.sql
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ system_design.png
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_etl.py
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ .env.example

```
---
## ğŸ—ï¸ System Design

<p align="center">
  <a href="docs/system_design.png">
    <img src="docs/system_design.png" alt="ETL System Design" height = "400" width="800"/>
  </a>
</p>

---

## ğŸš€ Future Enhancements

- Incremental loading via watermarks
- Slowly Changing Dimensions (Type 1 / Type 2)
- Migration to PostgreSQL
- Orchestration with Airflow
- Data quality metrics dashboard

---

## ğŸ§ª Design Principles

- Separation of concerns
- Deterministic and rerunnable pipelines
- Explicit schemas and grain
- Metadata-driven behavior
- Minimal dependencies

---

## ğŸ Status

âœ… Architecture finalized and locked  
ğŸš§ Implementation in progress

---

## ğŸ“Œ One-Line Summary

> A production-style ETL pipeline using Pandas and SQLite with full logging, metadata control, and dimensional modeling for retail analytics.
